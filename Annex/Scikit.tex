
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
 \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
    % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
    % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Les algorithmes sous scikit-learn}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Script:}
 \PY{c+c1}{\PYZsh{}   des algorithmes Random Forest (RF) Gradient Boosting (GBM),}
 \PY{c+c1}{\PYZsh{}   extrem Gradient Boosting (XGB) et Deep learning (DL) Sous Scikit\PYZhy{}learn}
 
 \PY{c+c1}{\PYZsh{} Description:}
 \PY{c+c1}{\PYZsh{}Ce script est dédié au développement des algorithme RF, GBM, XGB et DL,}
 \PY{c+c1}{\PYZsh{}le développement de ces algorithme suit étapes suivant:}
 \PY{c+c1}{\PYZsh{}  1\PYZhy{} Importation des bibliothéques et les données;}
 \PY{c+c1}{\PYZsh{}  2\PYZhy{} Création des fonctions d\PYZsq{}évaluation et d\PYZsq{}affichage des variables Important;}
 \PY{c+c1}{\PYZsh{}  3\PYZhy{} Développement des 3 algorithmes avec des paramètres Par défaut:}
 \PY{c+c1}{\PYZsh{}      3\PYZhy{}1). RF avec son évaluation et l\PYZsq{}affichage de ses variables important;}
 \PY{c+c1}{\PYZsh{}      3\PYZhy{}2). GBM avec son évaluation et l\PYZsq{}affichage de ses variables important;}
 \PY{c+c1}{\PYZsh{}      3\PYZhy{}3). XGB avec son évaluation et l\PYZsq{}affichage de ses variables important;}
 \PY{c+c1}{\PYZsh{}      3\PYZhy{}4). DL avec son évaluation.}
 \PY{c+c1}{\PYZsh{}  4\PYZhy{} Optimisation des hyperparamètres avec Random Search et Grid search;}
 \PY{c+c1}{\PYZsh{}  5\PYZhy{} Répétition de l\PYZsq{}étape 3 mais avec les paramètres séléctionné au l\PYZsq{}étape 4.}
 
 \PY{c+c1}{\PYZsh{} Version:}
 \PY{c+c1}{\PYZsh{}     Mohammed AMEKSA:Juin 2019Script Original}
\end{Verbatim}


    \hypertarget{importer-les-bibliothuxe8ques-et-les-donnuxe9es}{%
\setcounter{section}{1}
\subsection{Importer les bibliothèques et les
données}\label{importer-les-bibliothuxe8ques-et-les-donnuxe9es}}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} importer les bibliothèques }
 \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
 \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
 \PY{c+c1}{\PYZsh{} Importer le package de l\PYZsq{}algorithme random forest}
 \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{RandomForestRegressor}
 \PY{c+c1}{\PYZsh{} Importer le package de l\PYZsq{}algorithme gradient boosting}
 \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{import} \PY{n}{GradientBoostingRegressor}
 \PY{c+c1}{\PYZsh{} Importer le package de l\PYZsq{}algorithme xgboost}
 \PY{k+kn}{import} \PY{n+nn}{xgboost} \PY{k}{as} \PY{n+nn}{XGBRegressor}
 \PY{c+c1}{\PYZsh{} \PYZsh{} Importer le package de l\PYZsq{}algorithme d\PYZsq{}apprentissage profond}
 \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{neural\PYZus{}network}
 \PY{c+c1}{\PYZsh{} importer le package de standardisation pour deep learning}
 \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}  
 \PY{c+c1}{\PYZsh{}Pour l\PYZsq{}évaluation}
 \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{metrics}
 \PY{c+c1}{\PYZsh{}Optimisation des hyperparamètres du modèle}
 \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
 \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{RandomizedSearchCV}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Importer les deux fichiers de données}
 \PY{n}{dataset\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train\PYZhy{}Equil\PYZhy{}Lon\PYZhy{}Lat\PYZhy{}Hour\PYZhy{}Month\PYZhy{}RedVisi.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{n}{dataset\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test\PYZhy{}Equil\PYZhy{}Lon\PYZhy{}Lat\PYZhy{}Hour\PYZhy{}Month\PYZhy{}RedVisi.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 
 \PY{c+c1}{\PYZsh{}Détérminer pour chaque fichie les variables independants et le target}
 \PY{c+c1}{\PYZsh{}fichier d\PYZsq{}entrainement}
 \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{dataset\PYZus{}train}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}   
 \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{dataset\PYZus{}train}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}     
 \PY{c+c1}{\PYZsh{}fichier de test}
 \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{dataset\PYZus{}test}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}     
 \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{dataset\PYZus{}test}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \hypertarget{cruxe9ation-des-fonctions-duxe9valuation}{%
\subsection{Création des fonctions
d'évaluation}\label{cruxe9ation-des-fonctions-duxe9valuation}}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle en calculant les 6 indices de pérformance}
 \PY{c+c1}{\PYZsh{} coefficient de correlation, biais, variance, MAE, MSE et RMSE}
 \PY{k}{def} \PY{n+nf}{evaluate}\PY{p}{(}\PY{n}{model}\PY{p}{)}\PY{p}{:}
     \PY{c+c1}{\PYZsh{}prédire les données de test }
     \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} 
     \PY{c+c1}{\PYZsh{}évaluer le modèle}
     \PY{c+c1}{\PYZsh{}Calculer le coefficient de correlation }
     \PY{n}{cc}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{corrcoef}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
     \PY{c+c1}{\PYZsh{} Calculer le bias}
     \PY{n}{bias}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}
     \PY{c+c1}{\PYZsh{} Calculer la variance }
     \PY{n}{var} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}pred}\PY{p}{)}
     \PY{c+c1}{\PYZsh{} Calculer le MAE (mean absolute error)}
     \PY{n}{mea} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{mean\PYZus{}absolute\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
     \PY{c+c1}{\PYZsh{} Calculer MSE (mean squared error)}
     \PY{n}{mse} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
     \PY{c+c1}{\PYZsh{} Calculer RMSE (root mean squared error)}
     \PY{n}{rmse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{mse}\PY{p}{)}
     \PY{c+c1}{\PYZsh{}Afficher les résultats}
     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Coefficient de Correlation: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}}\PY{k}{cc})
     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Biais: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}}\PY{k}{bias})
     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance: }\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{\PYZsq{}}\PY{o}{\PYZpc{}}\PY{k}{var})
     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MAE: }\PY{l+s+si}{\PYZpc{}.5f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}}\PY{k}{mea})
     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSE: }\PY{l+s+si}{\PYZpc{}.5f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}}\PY{k}{mse})  
     \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RMSE: }\PY{l+s+si}{\PYZpc{}.5f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}}\PY{k}{rmse})  
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} traçer la courbe des variable importantes et afficher les 5 les plus importants}
 \PY{k}{def} \PY{n+nf}{importante\PYZus{}features}\PY{p}{(}\PY{n}{model}\PY{p}{)}\PY{p}{:}
     \PY{n}{imp}\PY{o}{=}\PY{n}{model}\PY{o}{.}\PY{n}{feature\PYZus{}importances\PYZus{}}
     \PY{n}{df\PYZus{}imp}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variables}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{} d}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
     \PY{n}{d}\PY{o}{=}\PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
     \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{imp}\PY{p}{)}\PY{p}{)}\PY{p}{:}
  \PY{n}{df\PYZus{}imp}\PY{o}{=}\PY{n}{df\PYZus{}imp}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variables}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{dataset\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,} 
   \PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{} d}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{imp}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
     \PY{n}{df\PYZus{}imp\PYZus{}sort}\PY{o}{=}\PY{n}{df\PYZus{}imp}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{} d}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{importance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
 \PY{n}{na\PYZus{}position}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{first}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
     \PY{n}{df\PYZus{}imp\PYZus{}sort}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{variables}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{plot}\PY{p}{(} \PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{barh}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
     \PY{n+nb}{print}\PY{p}{(}\PY{n}{df\PYZus{}imp\PYZus{}sort}\PY{o}{.}\PY{n}{tail}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
     
\end{Verbatim}


    \hypertarget{muxe9thodes-ensemblistes}{%
\subsection{1- Méthodes Ensemblistes}\label{muxe9thodes-ensemblistes}}

\hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

\hypertarget{par-duxe9faut}{%
\subsubsection{Par défaut}\label{par-duxe9faut}}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Création d\PYZsq{}un instance de random forest}
 \PY{n}{rf\PYZus{}defaut} \PY{o}{=} \PY{n}{RandomForestRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)} 
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{rf\PYZus{}defaut}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle Random Forest avec les paramètres par défauts}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{rf\PYZus{}defaut}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Random Forest }
 \PY{c+c1}{\PYZsh{} avec les paramètres par défauts}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{rf\PYZus{}defaut}\PY{p}{)}
\end{Verbatim}


    \hypertarget{optimisation-des-hyperparamuxe8tres-avec-grid-search-et-random-search}{%
\subsubsection{Optimisation des hyperparamètres avec Grid Search et
Random
Search}\label{optimisation-des-hyperparamuxe8tres-avec-grid-search-et-random-search}}

\textbf{les hyperparamètres les plus importants choisis pour Random
forest sont:} \\
* n\_estimators = nombre des arbres\\
* max\_features = nombre maximal d'entités prises en compte pour fractionner un nœud \\
* max\_depth = nombre maximum de niveaux dans chaque arbre de décision \\
*min\_samples\_split = Nombre minimal d'échantillons requis pour scinder un nœud\\
* min\_samples\_leaf = nombre min de points de données autorisés pour une feuille \\
* bootstrap = méthode d'échantillonnage des points de
données (avec ou sans remplacement)

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} parameters}
 \PY{c+c1}{\PYZsh{}  nombre des arbres pour random forest }
 \PY{n}{n\PYZus{}estimators\PYZus{}rf} \PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{start} \PY{o}{=} \PY{l+m+mi}{60}\PY{p}{,} \PY{n}{stop} \PY{o}{=} \PY{l+m+mi}{80}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} Nombre d\PYZsq{}observations à prendre en compte à chaque scission n/3 pour la régression}
 \PY{n}{features} \PY{o}{=} \PY{n}{dataset\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
 \PY{n}{max\PYZus{}features\PYZus{}rf} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sqrt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{int}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} Nombre maximum de niveaux dans une arbre }
 \PY{c+c1}{\PYZsh{} dans cette étude nous avons choisie une plage entre 1 et n/2 }
 \PY{c+c1}{\PYZsh{} avec n nombre d\PYZsq{}observation}
 \PY{n}{max\PYZus{}depth\PYZus{}rf} \PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{17}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} Méthode d\PYZsq{}échentillonage des points}
 \PY{n}{bootstrap\PYZus{}rf} \PY{o}{=} \PY{p}{[}\PY{k+kc}{True}\PY{p}{,} \PY{k+kc}{False}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} Nombre minimal d\PYZsq{}échantillons requis pour scinder un nœud}
 \PY{n}{min\PYZus{}samples\PYZus{}split\PYZus{}rf} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} nombre min de points de données autorisés pour une feuille}
 \PY{n}{min\PYZus{}samples\PYZus{}leaf\PYZus{}rf}\PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \textbf{Random Search}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Création de dictionnaire (random grid)}
 \PY{n}{rf\PYZus{}random\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{n\PYZus{}estimators\PYZus{}rf}\PY{p}{,}
\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}features\PYZus{}rf}\PY{p}{,}
\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}depth\PYZus{}rf}\PY{p}{,}
\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bootstrap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{bootstrap\PYZus{}rf}\PY{p}{,}
\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}samples\PYZus{}split\PYZus{}rf}\PY{p}{,}
\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}samples\PYZus{}leaf\PYZus{}rf}\PY{p}{,}
    \PY{p}{\PYZcb{}}
 \PY{c+c1}{\PYZsh{} Utiliser ce dictionaire (random grid) pour chercher la combinaison}
 \PY{c+c1}{\PYZsh{} des hyperparamètres optimale}
 \PY{n}{rf\PYZus{}random\PYZus{}search} \PY{o}{=} \PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{RandomForestRegressor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{param\PYZus{}distributions} \PY{o}{=} \PY{n}{rf\PYZus{}random\PYZus{}grid}\PY{p}{,}
    \PY{n}{n\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,} 
    \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,} 
    \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} 
    \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{rf\PYZus{}random\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle Random Forest avec les paramètres choisis }
 \PY{c+c1}{\PYZsh{} par random search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{rf\PYZus{}random\PYZus{}search}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Random Forest }
 \PY{c+c1}{\PYZsh{} avec les paramètres choisis par random search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{rf\PYZus{}random\PYZus{}search}\PY{p}{)}
\end{Verbatim}


    \textbf{Grid Search} \\
    Vue que Grid Search ça prend du temps nous avons
éssayer de faire l'optimisation comme suit: \\
* premièrement nous avons optimiser (n\_estimators) et (max\_features)\\
* utiliser les valeurs optimales de n\_estimators et max\_features pour optimiser min\_samples\_split et min\_samples\_leaf \\
* Utiliser les quatres précidents pour optimiser le reste des paramètres.

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Optimiser n\PYZus{}estimators and max\PYZus{}features, puis utiliser les valeur optimales}
 \PY{c+c1}{\PYZsh{} choisis pour optimiser les autres}
 \PY{n}{param\PYZus{}grid\PYZus{}rf} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{n\PYZus{}estimators\PYZus{}rf}\PY{p}{,}
  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}features\PYZus{}rf}\PY{p}{,}
 \PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}forest} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{RandomForestRegressor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
\PY{n}{param\PYZus{}grid\PYZus{}rf}\PY{p}{,}      
\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
 \PY{c+c1}{\PYZsh{}pour que les résultats soit toujours les mêmes}
\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
      \PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les mielleurs hyperparamètres choisis}
 \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Random Forest avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Random Forest avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Optimiser min\PYZus{}samples\PYZus{}split et min\PYZus{}samples\PYZus{}leaf, puis utiliser}
 \PY{c+c1}{\PYZsh{} les valeur optimales choisis pour optimiser les autres}
 \PY{n}{param\PYZus{}grid\PYZus{}rf} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}samples\PYZus{}split\PYZus{}rf}\PY{p}{,}
  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{min\PYZus{}samples\PYZus{}leaf\PYZus{}rf}\PY{p}{,}
 \PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}forest} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{RandomForestRegressor}\PY{p}{(}
    \PY{c+c1}{\PYZsh{} ici nous spécifions les valeurs choisis}
    \PY{c+c1}{\PYZsh{}pour n\PYZus{}estimators et max\PYZus{}feautures}
    \PY{c+c1}{\PYZsh{} notre cas les valeurs suivants sont les optimales}
      \PY{n}{n\PYZus{}estimators}\PY{o}{=} \PY{l+m+mi}{75} \PY{p}{,}
      \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{11}
  \PY{p}{)}\PY{p}{,}
\PY{n}{param\PYZus{}grid\PYZus{}rf}\PY{p}{,}      
\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{c+c1}{\PYZsh{}pour que les résultats soit toujours les mêmes}
\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
      \PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les mielleurs hyperparamètres choisis}
 \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Random Forest avec les paramètres }
 \PY{c+c1}{\PYZsh{} choisis par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Random Forest }
 \PY{c+c1}{\PYZsh{} avec les paramètres choisis par grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} De même nous avons essayer d\PYZsq{}optimiser max\PYZus{}depth et bootstrap, }
 \PY{c+c1}{\PYZsh{} puis utiliser les valeur optimales choisis pour optimiser les autres}
 \PY{n}{param\PYZus{}grid\PYZus{}rf} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}
  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}depth\PYZus{}rf}\PY{p}{,}
  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bootstrap}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{bootstrap\PYZus{}rf}\PY{p}{,}
 \PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}forest} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{RandomForestRegressor}\PY{p}{(}
\PY{c+c1}{\PYZsh{} ici nous spécifions les valeurs choisis }
\PY{c+c1}{\PYZsh{} pour n\PYZus{}estimators et max\PYZus{}feautures}
\PY{c+c1}{\PYZsh{} notre cas les valeurs suivants sont les optimales}
      \PY{n}{n\PYZus{}estimators}\PY{o}{=} \PY{l+m+mi}{75} \PY{p}{,}
      \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{11}\PY{p}{,}
      \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
      \PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
  \PY{p}{)}\PY{p}{,}
\PY{n}{param\PYZus{}grid\PYZus{}rf}\PY{p}{,}      
\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
\PY{c+c1}{\PYZsh{}pour que les résultats soit toujours les mêmes}
\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
      \PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les mielleurs hyperparamètres choisis}
 \PY{n+nb}{print}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Random Forest avec les paramètres choisis }
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Random Forest avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}forest}\PY{p}{)}
\end{Verbatim}


    \hypertarget{gradient-boosting-machine}{%
\subsection{Gradient Boosting Machine}\label{gradient-boosting-machine}}

\hypertarget{par-duxe9faut}{%
\subsubsection{Par défaut}\label{par-duxe9faut}}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Création d\PYZsq{}un instance de gradient boosting machine}
 \PY{n}{gbm\PYZus{}defaut} \PY{o}{=} \PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{gbm\PYZus{}defaut}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle gradient boosting machine avec les paramètres }
 \PY{c+c1}{\PYZsh{} par défauts}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{gbm\PYZus{}defaut}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle gradient boosting machine }
 \PY{c+c1}{\PYZsh{} avec les paramètres par défauts}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{gbm\PYZus{}defaut}\PY{p}{)}
\end{Verbatim}


    \hypertarget{optimisation-des-hyperparamuxe8tres-avec-grid-search-et-random-search}{%
\subsubsection{Optimisation des hyperparamètres avec Grid Search et
Random
Search}\label{optimisation-des-hyperparamuxe8tres-avec-grid-search-et-random-search}}

\textbf{les hyperparamètres les plus importants choisis pour Gradient
Boosting Machine sont:}

Les paramètres généraux peuvent être divisés en 3 catégories: \\
* Paramètres spécifiques aux arbres: ils affectent chaque arbre individuel
du modèle.\\
* Paramètres d'amplification: ils affectent l'opération
d'amplification dans le modèle. \\
* Paramètres divers: Autres paramètres
pour le fonctionnement général.\\

Les paramètres utilisés pour définir un arbre \\
* min\_samples\_split: nombre minimal d'échantillons (ou d'observations) nécessaires dans un nœud à prendre en compte pour la scission.\\
* min\_samples\_leaf: minimum d'échantillons (ou d'observations) requis dans un nœud terminal ou une
feuille\\
* max\_depth: profondeur maximale d'un arbre.\\
* max\_feature: nombre d'observations à prendre en compte lors de la recherche du meilleur split.\\

paramètres de gestion du boosting:\\
* n\_estimators: nombre d'arbres séquentiels à modéliser\\
* learning\_rate: détermine l'impact de chaque arbre sur le résultat final

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Les intérvale des paramètres}
 \PY{c+c1}{\PYZsh{} Nombre d\PYZsq{}arbres}
 \PY{n}{n\PYZus{}estimators\PYZus{}gbm} \PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{start} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{stop} \PY{o}{=} \PY{l+m+mi}{200}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} nombre des observations à considirer pour un split }
 \PY{c+c1}{\PYZsh{}sqrt(n) pour classification et n/3 for régression}
 \PY{n}{features} \PY{o}{=} \PY{n}{dataset\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} 
 \PY{n}{max\PYZus{}features\PYZus{}gbm} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sqrt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{int}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} profondeur maximale d\PYZsq{}un arbre}
 \PY{n}{max\PYZus{}depth\PYZus{}gbm} \PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} nombre minimal d\PYZsq{}échantillons (ou d\PYZsq{}observations) nécessaires }
 \PY{c+c1}{\PYZsh{} dans un nœud à prendre en compte pour la scission}
 \PY{n}{min\PYZus{}samples\PYZus{}split\PYZus{}gbm} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Taux d\PYZsq{}apprentissage}
 \PY{n}{learning\PYZus{}rate\PYZus{}gbm} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.025}\PY{p}{,} \PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.075}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.15}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}
 \PY{n}{min\PYZus{}samples\PYZus{}leaf\PYZus{}gbm}\PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \textbf{Random Search}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Création de dictionnaire (random grid)}
 \PY{n}{GBM\PYZus{}random\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}features\PYZus{}gbm}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}depth\PYZus{}gbm}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}samples\PYZus{}split\PYZus{}gbm}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{min\PYZus{}samples\PYZus{}leaf\PYZus{}gbm}\PY{p}{,}
      
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{n\PYZus{}estimators\PYZus{}gbm}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{learning\PYZus{}rate\PYZus{}gbm}\PY{p}{,}
     \PY{p}{\PYZcb{}}
 
 \PY{c+c1}{\PYZsh{} Utiliser ce dictionaire (random grid) pour chercher la combinaison }
 \PY{c+c1}{\PYZsh{} des hyperparamètres optimale}
 \PY{n}{random\PYZus{}search\PYZus{}GBM} \PY{o}{=} \PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{param\PYZus{}distributions} \PY{o}{=} \PY{n}{GBM\PYZus{}random\PYZus{}grid}\PY{p}{,}
    \PY{n}{n\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,} 
    \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,} 
    \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} 
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
    \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}apprentissage}
 \PY{n}{random\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle Gradient Boosting Machine avec les paramètres choisis }
 \PY{c+c1}{\PYZsh{} par random search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{random\PYZus{}search\PYZus{}GBM}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Gradient Boosting Machine avec}
 \PY{c+c1}{\PYZsh{} les paramètres choisis par random search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{random\PYZus{}search\PYZus{}GBM}\PY{p}{)}
\end{Verbatim}


    \textbf{Grid Search}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{param\PYZus{}grid\PYZus{}gbm} \PY{o}{=} \PY{p}{[}
     \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}features\PYZus{}gbm}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}gbm}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Gradient Boosting Machine avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Gradient Boosting Machine avec}
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{param\PYZus{}grid\PYZus{}gbm} \PY{o}{=} \PY{p}{[}
     \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}depth\PYZus{}gbm}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec les paramètres séléctionné précédement}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}gbm}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 
 \PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Gradient Boosting Machine avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Gradient Boosting Machine avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{param\PYZus{}grid\PYZus{}gbm} \PY{o}{=} \PY{p}{[}
     \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}split}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{min\PYZus{}samples\PYZus{}split\PYZus{}gbm}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec les paramètres séléctionné précédement}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{max\PYZus{}features} \PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,} 
  \PY{n}{max\PYZus{}depth} \PY{o}{=}\PY{l+m+mi}{10} \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}gbm}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Gradient Boosting Machine avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Gradient Boosting Machine avec}
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{param\PYZus{}grid\PYZus{}gbm} \PY{o}{=} \PY{p}{[}
     \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{min\PYZus{}samples\PYZus{}leaf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{min\PYZus{}samples\PYZus{}leaf\PYZus{}gbm}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec les paramètres séléctionné précédement}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{max\PYZus{}features}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
  \PY{n}{max\PYZus{}depth}\PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}
  \PY{n}{min\PYZus{}samples\PYZus{}split} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,} \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}gbm}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Gradient Boosting Machine avec les paramètres choisis }
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Gradient Boosting Machine avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{param\PYZus{}grid\PYZus{}gbm} \PY{o}{=} \PY{p}{[}
     \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{n\PYZus{}estimators\PYZus{}gbm} \PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec les paramètres séléctionné précédement}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}} \PY{p}{,}
  \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}
  \PY{n}{min\PYZus{}samples\PYZus{}split} \PY{o}{=}\PY{l+m+mi}{3} \PY{p}{,}
  \PY{n}{min\PYZus{}samples\PYZus{}leaf} \PY{o}{=} \PY{l+m+mi}{1} \PY{p}{,}
 \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}gbm}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Gradient Boosting Machine avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Gradient Boosting Machine avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{param\PYZus{}grid\PYZus{}gbm} \PY{o}{=} \PY{p}{[}
     \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{learning\PYZus{}rate\PYZus{}gbm}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec les paramètres séléctionné précédement}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{GradientBoostingRegressor}\PY{p}{(}\PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
  \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}
  \PY{n}{min\PYZus{}samples\PYZus{}split} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,}
  \PY{n}{min\PYZus{}samples\PYZus{}leaf} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}
  \PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{l+m+mi}{200}\PY{p}{,}
 \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}gbm}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle Gradient Boosting Machine avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle Gradient Boosting Machine avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}GBM}\PY{p}{)}
\end{Verbatim}


    \hypertarget{extrem-gradient-boosting}{%
\subsection{eXtrem Gradient Boosting}\label{extrem-gradient-boosting}}

\hypertarget{par-duxe9faut}{%
\subsubsection{Par défaut}\label{par-duxe9faut}}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Création d\PYZsq{}un instance d\PYZsq{}eXtrem Gradient Boosting}
 \PY{n}{xgb\PYZus{}defaut} \PY{o}{=} \PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{xgb\PYZus{}defaut}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres}
 \PY{c+c1}{\PYZsh{} par défauts}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{xgb\PYZus{}defaut}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting }
 \PY{c+c1}{\PYZsh{} avec les paramètres par défauts}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{xgb\PYZus{}defaut}\PY{p}{)}
\end{Verbatim}


    \hypertarget{optimisation-des-hyperparamuxe8tres-avec-grid-search-et-random-search}{%
\subsubsection{Optimisation des hyperparamètres avec Grid Search et
Random
Search}\label{optimisation-des-hyperparamuxe8tres-avec-grid-search-et-random-search}}

\textbf{les hyperparamètres les plus importants choisis pour Gradient
Boosting Machine sont:}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Les intérvalles de variations des hyperparamètres choisis pour XGB}
 \PY{c+c1}{\PYZsh{} Le taux d\PYZsq{}apprentissage souvent entre 0.1 et 0.01 }
 \PY{c+c1}{\PYZsh{} or on peut essayer avec 0.15 0.2}
 \PY{n}{learning\PYZus{}rate\PYZus{}xgb} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.025}\PY{p}{,} \PY{l+m+mf}{0.05}\PY{p}{,} \PY{l+m+mf}{0.075}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.15}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} nombre d\PYZsq{}arbres, généralemnet 100 si la taille des données est elevé}
 \PY{n}{n\PYZus{}estimators\PYZus{}xgb} \PY{o}{=}  \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{start} \PY{o}{=} \PY{l+m+mi}{100}\PY{p}{,} \PY{n}{stop} \PY{o}{=} \PY{l+m+mi}{200}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} profondeur des arbres utilisé dans le modèle}
 \PY{n}{max\PYZus{}depth\PYZus{}xgb} \PY{o}{=} \PY{p}{[}\PY{n+nb}{int}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{,} \PY{n}{num} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} \PYZpc{} des ligne séléctionés pour construire chaque arbre, }
 \PY{c+c1}{\PYZsh{} généralement prend des valeurs entre 0.8 et 1}
 \PY{n}{subsample\PYZus{}xgb} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{/}\PY{l+m+mf}{100.0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{80}\PY{p}{,}\PY{l+m+mi}{101}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} nombre de colonne utilisé par chaque arbre, }
 \PY{c+c1}{\PYZsh{} généralement prend des valeurs entre 0.3 et 0.8 }
 \PY{c+c1}{\PYZsh{} pour des problèmes où y a un grand nombre des colonnes}
 \PY{c+c1}{\PYZsh{} et entre 0.8 et 1 pour des problèmes avec un peu de colonnes }
 \PY{n}{colsample\PYZus{}bytree\PYZus{}xgb} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{/}\PY{l+m+mf}{100.0} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{80}\PY{p}{,}\PY{l+m+mi}{101}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{]}
 \PY{c+c1}{\PYZsh{} il agit comme un paramètre de régularisation. 0, 1 ou 5}
 \PY{n}{gamma\PYZus{}xgb} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{]}
\end{Verbatim}


    \textbf{Random Search}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Création de dictionnaire (random grid)}
 \PY{n}{random\PYZus{}grid\PYZus{}XGB} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}depth\PYZus{}xgb}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{gamma\PYZus{}xgb}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{colsample\PYZus{}bytree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{colsample\PYZus{}bytree\PYZus{}xgb}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{learning\PYZus{}rate\PYZus{}xgb}\PY{p}{,}
      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{n\PYZus{}estimators\PYZus{}xgb}\PY{p}{,}
     \PY{p}{\PYZcb{}}
 \PY{c+c1}{\PYZsh{} Utiliser ce dictionaire (random grid) pour chercher la combinaison }
 \PY{c+c1}{\PYZsh{} des hyperparamètres optimales}
 \PY{n}{random\PYZus{}search\PYZus{}XGB} \PY{o}{=} \PY{n}{RandomizedSearchCV}\PY{p}{(}\PY{n}{estimator} \PY{o}{=} \PY{n}{XGBRegressor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{param\PYZus{}distributions} \PY{o}{=} \PY{n}{random\PYZus{}grid\PYZus{}XGB}\PY{p}{,}
    \PY{n}{n\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,} 
    \PY{n}{cv} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{,} 
    \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} 
    \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
    \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}apprentissage}
 \PY{n}{random\PYZus{}search\PYZus{}XGB}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis }
 \PY{c+c1}{\PYZsh{} par random search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{random\PYZus{}search\PYZus{}XGB}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting}
 \PY{c+c1}{\PYZsh{} avec les paramètres choisis par random search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{random\PYZus{}search\PYZus{}XGB}\PY{p}{)}
\end{Verbatim}


    \textbf{Grid Search}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Sélectionne n\PYZus{}estimators à optimiser en premier temps}
 \PY{n}{param\PYZus{}grid\PYZus{}xgb} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{n\PYZus{}estimators\PYZus{}xgb}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle }
 \PY{n}{grid\PYZus{}search\PYZus{}XGB} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{XGBRegressor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}xgb}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting avec}
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Sélectionne max\PYZus{}depth à optimiser }
 \PY{n}{param\PYZus{}grid\PYZus{}xgb} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max\PYZus{}depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{max\PYZus{}depth\PYZus{}xgb}\PY{p}{,} \PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec le nombre d\PYZsq{}arbres séléctionné précédement}
 \PY{c+c1}{\PYZsh{} (dans notre cas =190)}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{l+m+mi}{190}\PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}xgb}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
    \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis }
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting avec}
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Sélectionne subsample à optimiser }
 \PY{n}{param\PYZus{}grid\PYZus{}xgb} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subsample}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{subsample\PYZus{}xgb}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec le nombre d\PYZsq{}arbres séléctionné précédement }
 \PY{c+c1}{\PYZsh{}(dans notre cas n\PYZus{}estimators=190 et max\PYZus{}depth=11)}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{n\PYZus{}estimators} \PY{o}{=}\PY{l+m+mi}{190} \PY{p}{,}
   \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{11}\PY{p}{,}
  \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}xgb}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis }
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Sélectionner gamma à optimiser}
 \PY{n}{param\PYZus{}grid\PYZus{}xgb} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{gamma\PYZus{}xgb}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec le nombre d\PYZsq{}arbres séléctionné précédement }
 \PY{c+c1}{\PYZsh{}(dans notre cas n\PYZus{}estimators=190 et max\PYZus{}depth=11 subsample est par défaut)}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{l+m+mi}{190} \PY{p}{,}
   \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{11}\PY{p}{,}
   \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}xgb}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Sélectionner colsample\PYZus{}bytree à optimiser}
 \PY{n}{param\PYZus{}grid\PYZus{}xgb} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{colsample\PYZus{}bytree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{colsample\PYZus{}bytree\PYZus{}xgb}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec le nombre d\PYZsq{}arbres séléctionné précédement }
 \PY{c+c1}{\PYZsh{}(dans notre cas n\PYZus{}estimators=190, max\PYZus{}depth=11, subsample est par défaut}
 \PY{c+c1}{\PYZsh{} et gamma=3)}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{l+m+mi}{190}\PY{p}{,}
   \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{11}\PY{p}{,}
   \PY{n}{gamma} \PY{o}{=} \PY{l+m+mi}{3} \PY{p}{,}
   \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}xgb}\PY{p}{,} 
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting avec}
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Sélectionner learning\PYZus{}rate à optimiser }
 \PY{n}{param\PYZus{}grid\PYZus{}xgb} \PY{o}{=} \PY{p}{[}
 \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{learning\PYZus{}rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{learning\PYZus{}rate\PYZus{}xgb}\PY{p}{,}\PY{p}{\PYZcb{}}
 \PY{p}{]}
 \PY{c+c1}{\PYZsh{} Création du modèle avec le nombre d\PYZsq{}arbres séléctionné précédement }
 \PY{c+c1}{\PYZsh{}(dans notre cas n\PYZus{}estimators=190, max\PYZus{}depth=11, subsample et }
 \PY{c+c1}{\PYZsh{} colsample\PYZus{}bytree sont par défaut et gamma=3)}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{XGBRegressor}\PY{p}{(}\PY{n}{n\PYZus{}estimators} \PY{o}{=}\PY{l+m+mi}{190} \PY{p}{,}
   \PY{n}{max\PYZus{}depth} \PY{o}{=} \PY{l+m+mi}{11}\PY{p}{,}
   \PY{n}{gamma} \PY{o}{=} \PY{l+m+mi}{3} \PY{p}{,}
  \PY{p}{)}\PY{p}{,}
     \PY{n}{param\PYZus{}grid\PYZus{}xgb}\PY{p}{,}
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} 
     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajustement du modèle}
 \PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} évaluation du modèle eXtrem Gradient Boosting avec les paramètres choisis}
 \PY{c+c1}{\PYZsh{} par Grid search}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Afficher les variables importants du modèle eXtrem Gradient Boosting avec }
 \PY{c+c1}{\PYZsh{} les paramètres choisis par Grid search}
 \PY{n}{importante\PYZus{}features}\PY{p}{(}\PY{n}{grid\PYZus{}search\PYZus{}XGB}\PY{p}{)}
\end{Verbatim}


    \hypertarget{apprentissage-profond}{%
\subsection{2- Apprentissage profond}\label{apprentissage-profond}}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Ici, nous avons standardisé les données en faisant appel }
 \PY{c+c1}{\PYZsh{} à un instance de StandardScaler}
 \PY{c+c1}{\PYZsh{} cette instance suit la régle \PYZdq{}x\PYZhy{}men/std\PYZdq{}}
 \PY{n}{scaler} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}  
 \PY{n}{scaler}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
 
 \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}  
 \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}  
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Création d\PYZsq{}un instance de MLPRegressor avec les paramètres par défauts}
 \PY{n}{MLP\PYZus{}defaut}\PY{o}{=}\PY{n}{neural\PYZus{}network}\PY{o}{.}\PY{n}{MLPRegressor}\PY{p}{(}\PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{)}
 \PY{c+c1}{\PYZsh{} Ajuster le modèle sur les données d\PYZsq{}entrainement}
 \PY{n}{MLP\PYZus{}defaut}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle MLP avec les paramètres choisis par défauts}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{MLP\PYZus{}defaut}\PY{p}{)}
\end{Verbatim}


    \begin{itemize}
\tightlist
\item
  Ensuite, nous créons une instance du modèle Multilayer perceptrons
  (réseau de neurons), nous définirons hidden\_layer\_sizes. Pour ce
  paramètre, nous transmettrons une paire composée du nombre de neurones
  que nous voulons au niveau de chaque couche, la nième entrée du tuple
  représente le nombre de neurones de la nième couche du modèle MLP.
\item
  pour la simplicité, nous choisirons 2 couches avec le même nombre de
  neurones (68 nombre d'entrées fois 2) ainsi que 100 itérations
  maximum.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{MLP\PYZus{}manuel}\PY{o}{=}\PY{n}{neural\PYZus{}network}\PY{o}{.}\PY{n}{MLPRegressor}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}sizes}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{68}\PY{p}{,}\PY{l+m+mi}{68}\PY{p}{)}\PY{p}{,}
     \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,}
     \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
     \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{,}
    \PY{p}{)}
 \PY{n}{MLP\PYZus{}manuel}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} évaluation du modèle MLP avec les paramètres choisis manuellement}
 \PY{n}{evaluate}\PY{p}{(}\PY{n}{MLP\PYZus{}manuel}\PY{p}{)}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
